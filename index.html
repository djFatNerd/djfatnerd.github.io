<!DOCTYPE html>
<html>

  <head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-RFR2GFKT75"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-RFR2GFKT75');
  </script> -->
  <!-- Google Tag Manager -->
  <!-- <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  })(window,document,'script','dataLayer','GTM-NC38VTD');</script> -->
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


  <meta name="viewport" content="width=device-width, initial-scale=1">
	  
	<!-- <meta name="google-site-verification" content="u_--0s_Uw50r4zCgRrzltkmFhNCduYOJwPGlKVTuNJU" /> -->

  <title>Enxin Song 宋恩欣</title>
  <meta name="description" lang="en" content="This is an academic website for Enxin Song">
  <meta name="keywords" lang="en" content="Enxin Song." />
  

  <link rel="shortcut icon" href="old_web/static/imgs/favicon.ico">
  <link rel="stylesheet" href="main.css">
  <link rel="stylesheet" href="css/custom.css">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="stylesheet" href="css/academicons.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">



  <script src="js/jquery-2.1.3.min.js"> </script>

  <!--[if lt IE 9]>
<script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/r29/html5.min.js">
</script>
<![endif]-->

  <script type="text/javascript">
    function ShowHide(divId, linkDivId) {
        if(document.getElementById(divId).style.display == 'none') {
            document.getElementById(divId).style.display='block';
            document.getElementById(linkDivId).style.display='none'; // Hide the "read more" link
        }
        else {
            document.getElementById(divId).style.display = 'none';
        }
    }
  </script>

  <style>
   
    li.indented {
        position: relative;
        padding-left: 90px; 
    }

    li.indented::before {
        content: attr(data-date);
        position: absolute;
        left: 0;
        top: 0;
        white-space: nowrap;
    }
  </style>
  
</head>


<body>

    <!-- Google Tag Manager (noscript) -->
<!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NC38VTD"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> -->
<!-- End Google Tag Manager (noscript) -->

    <header class="site-header">

  <div class="wrapper">
<nav class="site-nav">
  <a href="#" class="menu-icon">
    <svg viewBox="0 0 18 15">
      <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
      <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
      <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
    </svg>
  </a>

  <div class="trigger">
    <a class="page-link" href="#bio"><b>Biography</b></a>
    <a class="page-link" href="#timeline"><b>Education</b></a>
    <a class="page-link" href="#timeline"><b>Experiences</b></a>
    <a class="page-link" href="#publications"><b>Publications</b></a>
    <a class="page-link" href="#awards"><b>Honors & Awards</b></a>
    <a class="page-link" href="#teaching"><b>Teaching </b></a>
    <a class="page-link" href="#academicservices"><b>Professional Services </b></a>
    <!-- <a class="page-link" href="#misc"><b>CV</b></a> -->
    <!--<a class="page-link" href="#otherprojects">Projects</a>-->
    <!-- Button for Posts -->
  </div>
</nav>
  </div>
</header>

  <div id="profile-cover" class="cover shallow-bg img-responsive"> 
    <div id="profile-namecard" class="profile-wrapper wrapper-light">
      <div id="my-pic" class="profile-col profile-col-1">
        <img id="profile-avatar" src="./images/self-portrait/me.JPEG" alt="Me" class="circle-img border-dark"-/>
      </div>
      <div id="my-contact" class="profile-col profile-col-2">
        <div id="my-name" class="text-grey-dark">
          Enxin Song (宋恩欣)<br>
        </div>
        <div id="my-title" class="text-grey">
          <!-- Math Undergrad @ ZJU 24'<br> -->
          Master Student @ ZJU
        </div>
        <!-- <div id="my-email" class="text-grey-light">
          wjzhao1203[at]gmail.com
        </div> -->
        <div class="social-media">
          <a href="https://scholar.google.com.hk/citations?user=sLqa-3oAAAAJ&hl=zh-CN" class="icon-button github">
          <i class="ai ai-google-scholar-square icon-github"></i>
            <span></span>
          </a>

          <a href="https://github.com/Espere-1119-Song" class="icon-button github">
            <i class="fa fa-github icon-github"></i>
            <span></span>
          </a>

          <a href="https://linkedin.com/in/enxinsong-57b471286/" class="icon-button linkedin">
            <i class="fa fa-linkedin icon-linkedin"></i>
            <span></span>
          </a>
          
          <a href="https://twitter.com/EnxinSong" class="icon-button twitter">
            <i class="fa fa-twitter icon-twitter"></i>
            <span></span>
          </a>
         
          <a href="mailto:enxin.23@intl.zju.edu.cn" class="icon-button github">
            <i class="fa fa-envelope icon-github"></i>
            <span></span>
          </a>
        </div>
      </div>

      <!-- <div id="my-desc" class="profile-col profile-col-2 hide">
        <div id="my-desc-title">
          Another me.
        </div>
        <div id="my-desc-content">
          I enjoy running, traveling, hiking, movies, music and so much more<br>
        </div>
      </div> -->
    </div>
  </div>
  <!-- <script type="text/javascript">
    function deepFeature()
    {
      $('#profile-avatar').attr('src', 'images/self-portrait/deep-me.jpg');
      $('#profile-avatar').removeClass('border-dark');
      $('#profile-avatar').addClass('border-bright');

      $('#profile-cover').removeClass('shallow-bg');
      $('#profile-cover').addClass('deep-bg');
      
      $('#profile-namecard').removeClass('wrapper-light');
      $('#profile-namecard').addClass('wrapper-dark');
      
      $('#my-desc').removeClass('hide');
      $('#my-contact').addClass('hide');    
      // console.log('Move in now...');
    }

    function shallowFeature()
    {
      $('#profile-avatar').attr('src', 'images/self-portrait/me.jpg');
      $('#profile-avatar').removeClass('border-bright');
      $('#profile-avatar').addClass('border-dark');

      $('#profile-cover').removeClass('deep-bg');
      $('#profile-cover').addClass('shallow-bg');
      
      $('#profile-namecard').removeClass('wrapper-dark');
      $('#profile-namecard').addClass('wrapper-light');
      
      $('#my-contact').removeClass('hide');
      $('#my-desc').addClass('hide');
      // console.log('Move out now...');
    }

    $(function() { 
      $('#my-pic').hover(deepFeature, shallowFeature);
    })

  </script> -->

<div class="page-content">
<div class="wrapper">
<div id="bio" class="bio">

  <h1 class="md-heading text-left">
    <i class="fa fa-id-card" aria-hidden="true"></i>
    About
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <p> 
      Enxin Song is a research intern at the University of California, San Diego (<a href="https://www.ucsd.edu/">UCSD</a>) under Prof. <a href="https://pages.ucsd.edu/~ztu/">Zhuowen Tu</a>. 
      She will receive her M.S. in March 2026 from <a href="https://zjui.intl.zju.edu.cn//">Zhejiang University</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=GhsXNiwAAAAJ&hl=zh-CN">Gaoang Wang</a> (<a href="https://cvnext.github.io/">CVNext Lab</a>), and holds a B.S. in Software Engineering from <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a>. 
      Previously, she was a research intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Microsoft Research Asia</a>.
      Enxin stays curious about the wider landscape of computer vision and deep learning, and actively seeks new collaboration opportunities.
      Her work centers on video understanding, highlighted by MovieChat, the first Large Mutli-Modal Model for hour-long video understanding.
      She has co-organized workshops and challenges on video understanding at CVPR 2024 and 2025.
      She is a highly self-motivated, and curious student applying to <strong>Ph.D. programs for 2026Fall</strong>.
      <br>
    </p>
  </div>
</div>


<div id="Experiences" class="bio">

  <h1 class="md-heading text-left">
    <i class="fa fa-tasks" aria-hidden="true"></i>
    Experiences
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <table>


      <tr>
        <tr>
          <td width="90%">
            <li>
              <strong>Feb. 2025 -- Present, University of California, San Diego (UCSD)</strong>, USA<br>
              &nbsp&nbsp&nbsp&nbsp <strong>Visiting Intern</strong>, Advised by <a href="https://pages.ucsd.edu/~ztu/">Prof. Zhuowen Tu</a>.<br>
            </li> 
          </td>
          <td width="10%">
            <img src="images/icon/UCSD.png" width="50%" style="display: block; margin-left: auto; margin-right: auto;">
          </td>
        </tr>

        <tr>
          <td width="90%">
            <li>
              <strong>Nov. 2023 -- May. 2024, Media Computing Group, Microsoft Research Lab - Asia</strong>, Beijing, China<br>
              &nbsp&nbsp&nbsp&nbsp <strong>Research Intern</strong>, Work on text-to-image generation<br>
              
            </li>
               
          </td>
          <td width="10%">
            <img src="images/icon/msra.jpg" width="40%" style="display: block; margin-left: auto; margin-right: auto;">
          </td>
        </tr>

    </tbody></table>
  </div>
  <br>
</div>


<div id="Education" class="bio">

  <h1 class="md-heading text-left">
    <i class="fa fa-graduation-cap" aria-hidden="true"></i>
    Education
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <table>
      <tbody><tr>
          <td width="90%">
              <br>
              <strong>M.S.</strong> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Sep. 2023 - Mar. 2026 (expected)  <br> 
              &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b>Zhejiang University</b>, Hangzhou, China.<br>
              &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 1/82, M.S. in Artificial Intelligence
                <br>
            </td>
          <td width="10%">
            <img src="images/icon/zju.png" width="40%" style="display: block; margin-left: auto; margin-right: auto;">
          </td>
        </tr>
        <tr>
          <td width="90%">
              <strong>B.S.</strong> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Sep. 2019 - Jun. 2023 <br>
              &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<b>Dalian University of Technology (DLUT)</b> , Dalian, China.<br>
              &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  21/385, B.S. in Software Engineering<br> 
          </td>
          <td width="10%">
            <img src="images/icon/dlut.png" width="40%" style="display: block; margin-left: auto; margin-right: auto;">
          </td>
        </tr>
    </tbody></table>
  </div>
  <br>
</div>





<div id="news" class="bio">
  <h1 class="md-heading text-left">
    <i class="fa fa-tasks" aria-hidden="true"></i>
    News
  </h1>  
  <ul>
    <li class="indented" data-date="[Jun, 2025]"> Our paper <i>Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark</i> is accepted by ICCV 2025 Findings. </li>
    <li class="indented" data-date="[Jun, 2025]"> Our paper <i>Bringing RNNs Back to Efficient Open-Ended Video Understanding</i> is accepted by ICCV 2025. </li>
    <li class="indented" data-date="[Apr, 2025]"> We are hosting two CVPR 2025 Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr25/track1a">LOVE Track 1A</a> and <a href="https://sites.google.com/view/loveucvpr25/track1b">LOVE Track 1B</a>. </li>
    <li class="indented" data-date="[Apr, 2025]"> We release <a href="https://arxiv.org/pdf/2504.14693">Video-MMLU</a>, a Massive Multi-Discipline Lecture Understanding Benchmark. </li>
    <li class="indented" data-date="[Mar, 2025]"> One paper accepted to CVPR 2025 workshop@<a href="https://sites.google.com/view/elvm/home">Efficient Large Vision Models</a>. </li>
    <li class="indented" data-date="[Jan, 2025]"> Our paper <i>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</i> is accepted by ICLR 2025. </li>
    <li class="indented" data-date="[Jan, 2025]"> Our paper <i>Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis</i> is accepted by ICLR 2025. </li>
    <!-- <li class="indented" data-date="[Oct, 2024]"> We submit some baseline (AuroraCap, MovieChat, MovieChat-Onevision) and some benchmark (VDC, MovieChat-1K) to <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a> (LLaVA team), which enables quick evaluation with just a single line of code. </li> -->
    <!-- <li class="indented" data-date="[Oct, 2024]"> We release a new version of MovieChat, which use LLaVA-OneVision as the base model instead of the original VideoLLaMA. The new version is available on <a href="https://github.com/rese1f/MovieChat/tree/main/MovieChat_Onevision">Github</a>. </li> -->
    <!-- <li class="indented" data-date="[Oct, 2024]"> Our paper <i>Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</i> is released, and the code is available at <a href="https://huggingface.co/MeissonFlow/Meissonic">website</a>. </li> 
    <li class="indented" data-date="[Oct, 2024]"> Our paper <i>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</i> is released, and the code is available at <a href="https://github.com/rese1f/aurora">website</a>. </li>   
    <li class="indented" data-date="[Jun, 2024]"> We finish CVPR 2024 Long-form Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr24/home">LOVEU</a> and presented a summary of the competition results offline.</a> </li>  
    <li class="indented" data-date="[May, 2024]"> Finish my research internship at Microsoft Research Asia (MSRA), Beijing.</a> </li>   
    <li class="indented" data-date="[Apr, 2024]"> We are hosting CVPR 2024 Long-form Video Understanding Challenge @ <a href="https://sites.google.com/view/loveucvpr24/track1">LOVEU</a> </li>  
    <li class="indented" data-date="[Apr, 2024]"> Our paper <i>MovieChat+: Question-aware Sparse Memory for Long Video Question Answering</i> is released, and the code is available at <a href="https://rese1f.github.io/MovieChat/">website</a>. </li>
    <li class="indented" data-date="[Feb, 2024]"> Our paper <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is accepted by Computer Vision and Pattern Recognition (CVPR), 2024. </li> -->
    <!-- <li class="indented" data-date="[Nov, 2023]"> Become a research intern at <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>, advised by principal researcher <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>. </li>
    <li class="indented" data-date="[Sep, 2023]"> Our paper <i>Devil in the Number: Towards Robust Multi-modality Data Filter</i> is accepted by ICCV 2023 workshop: <a href="https://www.datacomp.ai/">TNGCV-DataComp</a>. </li>
    <li class="indented" data-date="[Jul, 2023]"> Our project <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is released at <a href="https://rese1f.github.io/MovieChat/">website</a>. </li>
    <li class="indented" data-date="[Jun, 2023]"> I graduate from Dalian University of Technology. </li>
    <li class="indented" data-date="[Oct, 2022]"> Start my research on domain adaptation task for image caption, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. </li> -->
  </ul>
</div>

<br>

<div id="publications" class="publications">
  <h1 class="md-heading text-left">
    <i class="fa fa-file" aria-hidden="true"></i>
    Selected Publications and Manuscripts
  </h1>
  <p>
    * Equal contribution. † Project lead. ‡ Corresponding author.
  </p>
  <p>
    Also see <a href="https://scholar.google.com/citations?user=sLqa-3oAAAAJ&hl=en" target="_blank">Google Scholar</a>.
  </p>

  <div class="pub-list">
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/videommlu.png">
      </div>
      <div class="pub-right">
        <div class="title">
          Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai†</a>,
            <a class="author" href="https://weili-0234.github.io/">Weili Xu</a>,
            <a class="author" href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>,
            <a class="author" href="">Yuxuan Liu</a>,
            <a class="author" href="https://omerbt.github.io/">Gaoang Wang</a>,
          </font>
          </span>
        </div>
        <!-- <div class="publish">
          <span class="publisher">ICLR, 2025</span>
          <span class="place"></span>
        </div> -->
        <div class="tags">
          [<a class="tag" href="https://arxiv.org/abs/2504.14693"> <strong>Paper</strong></a>]
          [<a class="tag" href="https://enxinsong.com/Video-MMLU-web/"><strong>Website</strong></a>]
          [<a class="tag" href="https://huggingface.co/datasets/Enxin/Video-MMLU"><strong>Benchmark</strong></a>]
          [<a class="tag" href="https://github.com/Espere-1119-Song/Video-MMLU"><strong>Code</strong></a>]
          <img alt="NPM" src="https://img.shields.io/github/stars/Espere-1119-Song/Video-MMLU?style=social">
        </div>
        <span >Video-MMLU is a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. </span>
      </div>
    </div>
    
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/auroracap.JPG">
      </div>
      <div class="pub-right">
        <div class="title">
          AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai*†</a>,
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://yilundu.github.io/">Yilun Du</a>,
            <a class="author" href="https://cs.stanford.edu/~chenlin/">Chenlin Meng</a>,
            <a class="author" href="https://scholar.google.com/citations?user=WjF1dugAAAAJ">Vashisht Madhavan</a>,
            <a class="author" href="https://omerbt.github.io/">Omer Bar-Tal</a>,
            <a class="author" href="https://people.ece.uw.edu/hwang/">Jeng-Neng Hwang</a>,
            <a class="author" href="https://www.sainingxie.com/">Saining Xie</a>,
            <a class="author" href="https://nlp.stanford.edu/~manning/">Christopher D. Manning</a>
          </font>
          </span>
        </div>
        <div class="publish">
          <span class="publisher">ICLR, 2025</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          [<a class="tag" href="https://arxiv.org/pdf/2410.06366"> <strong>Paper</strong></a>]
          [<a class="tag" href="https://rese1f.github.io/aurora-web/"><strong>Website</strong></a>]
          [<a class="tag" href="https://huggingface.co/collections/wchai/auroracap-66d117ffe13bedda96702013"><strong>Model</strong></a>]
          [<a class="tag" href="https://huggingface.co/datasets/wchai/Video-Detailed-Caption"><strong>Benchmark</strong></a>]
          <!-- [<a class="tag" href="https://huggingface.co/datasets/wchai/AuroraCap-trainset"><strong>Dataset</strong></a>] -->
          [<a class="tag" href="https://github.com/rese1f/aurora"><strong>Code</strong></a>]
          <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/aurora?style=social">
        </div>
        <span >AuroraCap is a multimodal LLM designed for image and video detailed captioning. We also release VDC, the first benchmark for detailed video captioning.</span>
        <!-- <b><span >Conference on Neural Information Processing Systems (NeurIPS 2024)</span></b> -->
        <!-- <b><span style="color: red;"> Best Paper Award @ NeurIPS 2023 DLDE</span></b> -->
      </div>
    </div>

    <!-- <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/Fantasy.png">
      </div>
      <div class="pub-right">
        <div class="title">
          Fantasy: Transformer Meets Transformer in Text-to-Image Generation
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai*</a>,
            <a class="author" href="">Xun Guo</a>,
            <a class="author" href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>,
            <a class="author" href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>,
            <a class="author" href="">Yan Lu</a>
          </font>
        </div>
        <div class="publish">
          <span class="publisher">Preperint, 2024</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          [<a class="tag" href="https://openreview.net/forum?id=qL4nN6Ew7U"> <strong>Paper</strong></a>]
        </div>
        <span >Fantasy, an efficient text-to-image generation model marrying the decoder-only Large Language Models (LLMs) and transformer-based masked image modeling (MIM). </span>
      </div>
    </div> -->

    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="publications/moviechat.gif">
      </div>
      <div class="pub-right">
        <div class="title">
          MovieChat: From Dense Token to Sparse Memory for Long Video Understanding
        </div>
        <div class="desc">
        </div>
        <div class="authors">
          <font style="font-size: 11pt;">
            <a class="author" href="https://espere-1119-song.github.io/"><b>Enxin Song*</b></a>,
            <a class="author" href="https://rese1f.github.io/">Wenhao Chai*</a>,
            <a class="author" href="">Guanhong Wang*</a>,
            <a class="author" href="">Yucheng Zhang</a>,
            <a class="author" href="">Haoyang Zhou</a>,
            <a class="author" href="">Feiyang Wu</a>,
            <a class="author" href="">Xun Guo</a>,
            <a class="author" href="">Tian Ye</a>,
            <a class="author" href="">Yan Lu</a>,
            <a class="author" href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>,
            <a class="author" href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>

          </font>
        </div>
        <div class="publish">
          <span class="publisher">CVPR, 2024</span>
          <span class="place"></span>
        </div>
        <div class="tags">
          [<a class="tag" href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.html"> <strong>Paper</strong></a>]
          [<a class="tag" href="https://rese1f.github.io/MovieChat/"><strong>Website</strong></a>]
          [<a class="tag" href="https://huggingface.co/datasets/Enxin/MovieChat-1K_train"><strong>Benchmark</strong></a>]
          [<a class="tag" href="https://github.com/rese1f/MovieChat"><strong>Code</strong></a>]
          <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
        </div>
        <span >MovieChat achieves state-of-the-art performace in extra long video (more than 10K frames) understanding by introducing memory mechanism. </span>
      </div>
    </div>

    
  </div>
</div>
</div>


<div class="wrapper">
  <div id="academicservices" class="bio">
    <div class="bio-body" style="overflow:hidden;">
      <h1 class="md-heading text-left">
        <i class="fa fa-tasks" aria-hidden="true"></i>
        Professional Service
      </h1>  
      <li>
        <strong>Conference and Journal Refereeing:</strong> <br>
        &nbsp&nbsp&nbsp&nbsp NeurIPS 2025, PRCV 2025, CVPR 2025, ICLR 2025, TMM 2024, PRCV 2023<br>
      </li>
      <li>
        <strong>Workshop Organization:</strong> <br>
        &nbsp&nbsp&nbsp&nbsp Workshop on Long-form Video Understanding at CVPR 2025<br>
        &nbsp&nbsp&nbsp&nbsp Workshop on Long-form Video Understanding at CVPR 2024<br>
      </li>
    </div>
  </div>
</div>



<div class="wrapper">
  <div id="teaching" class="bio">
      <h1 class="md-heading text-left">
        <i class="fa fa-tasks" aria-hidden="true"></i>
        Teaching Assistant
      </h1>  
      <li>
        <strong>ECE 445 Senior Design (Undergraduate) - Sping 2024</strong><br>
        &nbsp&nbsp&nbsp&nbsp Teaching Assistant (TA), with <a href="https://person.zju.edu.cn/en/gaoangwang/">Prof. Gaoang Wang</a>
      </li>
  </div>
</div>

<div class="wrapper">
  <div id="awards" class="bio">
    <h1 class="md-heading text-left">
      <i class="fa fa-tasks" aria-hidden="true"></i>
      Selected Honors & Awards
    </h1>
    <ul>
      <li> National Scholarship, 2024 (Zhejiang University)</li>
      <li> National Scholarship, 2021 (Dalian University of Technology)</li>
    </ul>
  </div>
</div>



<footer class="site-footer">
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Personal Email:</li>
          <li><a href="mailto:">ensong [at] ucsd.edu</a></li>
          <li><a href="mailto:">enxin.23 [at] intl.zju.edu.cn</a></li>
          <li><a href="mailto:">enxin.song.dut [at] gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
          <a href="https://scholar.google.com.hk/citations?user=sLqa-3oAAAAJ&hl=zh-CN" class="icon-button github">
          <i class="ai ai-google-scholar-square icon-github"></i>
            <span></span>
          </a>

          <a href="https://github.com/Espere-1119-Song" class="icon-button github">
            <i class="fa fa-github icon-github"></i>
            <span></span>
          </a>

          <a href="https://linkedin.com/in/enxinsong-57b471286/" class="icon-button linkedin">
            <i class="fa fa-linkedin icon-linkedin"></i>
            <span></span>
          </a>
          
          <a href="https://twitter.com/EnxinSong" class="icon-button twitter">
            <i class="fa fa-twitter icon-twitter"></i>
            <span></span>
          </a>
         
          <a href="mailto:enxin.23@intl.zju.edu.cn" class="icon-button github">
            <i class="fa fa-envelope icon-github"></i>
            <span></span>
          </a>




      </div>
	<div class="footer-col footer-col-3">
    <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fespere-1119-song.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
    <p class="text">Last updated: Nov, 2024.<br>The style of this website is borrowed from <a href='https://www.hexianghu.com/'>Hexiang Hu's</a>.</p>
	</div>
  </div>
  </div>

</footer>
    <div class="back-to-top">Top</div>

<script type="text/javascript">
jQuery(document).ready(function() {
    var offset = 220;
    var duration = 500;
    jQuery(window).scroll(function() {
        if (jQuery(this).scrollTop() > offset) {
            jQuery('.back-to-top').fadeIn(duration);
        } else {
            jQuery('.back-to-top').fadeOut(duration);
    
        }
    });
    jQuery('.back-to-top').click(function(event) {
        event.preventDefault();
        jQuery('html, body').animate({scrollTop: 0}, duration);
        return false;
    })
});
</script>
  </body>

<!--
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60071442-1', 'auto');
  ga('send', 'pageview');
</script>
-->

<!--<script src="custom-pub.js"></script>-->
</html>
